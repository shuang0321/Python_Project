{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as plb\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matfn='train.mat'\n",
    "data=sio.loadmat(matfn)\n",
    "train_y=data['trainX'][...,784]\n",
    "train_x=data['trainX'][...,0:784]\n",
    "\n",
    "labels=np.zeros(shape=(len(train_y),10))\n",
    "for i in range(len(train_y)):\n",
    "    labels[i,(train_y[i]-1)]=1\n",
    "    \n",
    "train_x=train_x/255\n",
    "\n",
    "\n",
    "index=np.arange(len(train_x))\n",
    "np.random.shuffle(index)\n",
    "train_x=train_x[index]\n",
    "labels=labels[index]\n",
    "train_set=train_x[0:round(len(train_x)*0.8)]\n",
    "train_label=labels[0:round(len(train_x)*0.8)]\n",
    "valid_set=train_x[round(len(train_x)*0.8):]\n",
    "valid_label=labels[round(len(train_x)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP Network - Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "batch_size = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_1 = 256\n",
    "hidden_2 = 256\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "n_obs = train_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([hidden_1, hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([hidden_2, n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bp_network(x,weights,bias):\n",
    "    \n",
    "    # First Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x,weights['h1']),bias['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # Second Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Last Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict initialize\n",
    "pred = bp_network(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose loss function\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "prediction=tf.argmax(pred,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.120\n",
      "step 100, training accuracy 0.620\n",
      "step 200, training accuracy 0.780\n",
      "step 300, training accuracy 0.840\n",
      "step 400, training accuracy 0.660\n",
      "step 500, training accuracy 0.920\n",
      "Epoch: 1 cost=190.4891\n",
      "step 0, training accuracy 0.860\n",
      "step 100, training accuracy 0.920\n",
      "step 200, training accuracy 0.920\n",
      "step 300, training accuracy 0.980\n",
      "step 400, training accuracy 0.960\n",
      "step 500, training accuracy 0.880\n",
      "Epoch: 2 cost=44.9615\n",
      "step 0, training accuracy 0.880\n",
      "step 100, training accuracy 0.900\n",
      "step 200, training accuracy 0.860\n",
      "step 300, training accuracy 0.900\n",
      "step 400, training accuracy 0.920\n",
      "step 500, training accuracy 0.920\n",
      "Epoch: 3 cost=29.7683\n",
      "step 0, training accuracy 0.980\n",
      "step 100, training accuracy 0.880\n",
      "step 200, training accuracy 0.880\n",
      "step 300, training accuracy 0.920\n",
      "step 400, training accuracy 0.840\n",
      "step 500, training accuracy 0.880\n",
      "Epoch: 4 cost=22.0613\n",
      "step 0, training accuracy 0.960\n",
      "step 100, training accuracy 0.940\n",
      "step 200, training accuracy 0.920\n",
      "step 300, training accuracy 0.900\n",
      "step 400, training accuracy 0.900\n",
      "step 500, training accuracy 0.980\n",
      "Epoch: 5 cost=16.5963\n",
      "Model has completed 5 Epochs of Training\n"
     ]
    }
   ],
   "source": [
    "# Launch the session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Intialize all the variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training Epochs\n",
    "# Essentially the max amount of loops possible before we stop\n",
    "# May stop earlier if cost/loss limit was set\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Start with cost = 0.0\n",
    "    avg_cost = 0.0\n",
    "\n",
    "    # Convert total number of batches to integer\n",
    "    total_batch = int(n_obs/batch_size)\n",
    "\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "\n",
    "        # Grab the next batch of training data and labels\n",
    "        # batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_ind=np.random.randint(0,48000,50)\n",
    "        batch=[train_set[batch_ind],train_label[batch_ind]]\n",
    "        # Feed dictionary for optimization and loss value\n",
    "        # Returns a tuple, but we only need 'c' the cost\n",
    "        # So we set an underscore as a \"throwaway\"\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch[0], y: batch[1]})\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "        if i%100 == 0:  \n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x:batch[0], y: batch[1], keep_prob: 1.0})  \n",
    "            print (\"step %d, training accuracy %.3f\"%(i, train_accuracy))\n",
    "\n",
    "    print(\"Epoch: {} cost={:.4f}\".format(epoch+1,avg_cost))\n",
    "\n",
    "print(\"Model has completed {} Epochs of Training\".format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 8, 4, ..., 1, 4, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.eval(feed_dict={x: valid_set, keep_prob: 1.0}, session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93599999"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.eval(feed_dict={\n",
    "        x: train_set[0:5000], y: train_label[0:5000], keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_eval(y,model, X_test, Y_test):\n",
    "    acc_value = tf.reduce_mean(keras.metrics.categorical_accuracy(y, pred))\n",
    "    cur_acc = acc_value.eval(feed_dict={x: X_test,\n",
    "                           y: Y_test,\n",
    "                           keras.backend.learning_phase(): 0})\n",
    "    return cur_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 待研究\n",
    "def batch_eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93685418"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正常的training accuracy\n",
    "model_eval(y,pred,train_set,train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fast gradient sign method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad, = tf.gradients(loss,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adv_x = tf.stop_gradient(x + epsilon * tf.sign(grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### batch_eval 函数解构（没完）\n",
    "#eval_params = {'batch_size': FLAGS.batch_size}\n",
    "X_test_adv, = batch_eval(sess, [x], [adv_x], [X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###model_eval 函数解构(部分完成） 找出X_test_adv的结果\n",
    "# adv example 的 training accuracy\n",
    "model_eval(y, pred, X_test_adv, Y_test) #正常完成batch_eval函数应该就能得到这个accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对抗训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 完成adv example的accuracy后，完成fast_gradient函数，然后下面进行对抗训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_new = bp_network(x, weights, biases)\n",
    "\n",
    "adv_x_new = fast_gradient(x, predictions_2, eps=0.3)\n",
    "pred_adv = bp_network(adv_x, weights, biases)\n",
    "\n",
    "model_eval(y,pred_new,train_set,train_label) \n",
    "model_eval(y,pred_adv,train_set,train_label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 以下是备用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keras bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 7s - loss: 0.3689 - acc: 0.9017     \n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 5s - loss: 0.1644 - acc: 0.9532     \n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 9s - loss: 0.1147 - acc: 0.9676     \n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 5s - loss: 0.0868 - acc: 0.9754     \n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 5s - loss: 0.0668 - acc: 0.9815     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132a4aeb8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units = 200, input_dim = 784, activation = 'tanh'))\n",
    "model.add(Dense(units = 10, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(train_set,train_label, epochs = 5, batch_size = 80) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 可能有用的函数。。 可以等之前的code跑通了再把这些放到一起\n",
    "def model_loss(y,model):\n",
    "    op = model.op\n",
    "    if \"softmax\" in str(op).lower():\n",
    "        logits, = op.inputs\n",
    "    else:\n",
    "        logits = model\n",
    "    out  = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y)\n",
    "    return out\n",
    "\n",
    "def fast_gradient(x,pred,epsilon):\n",
    "    \"\"\"\n",
    "    :param x: the input placeholder\n",
    "    :param pred: the model's output tensor\n",
    "    :param epsilon: input variation parameter\n",
    "    \n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute loss\n",
    "    y = tf.to_float(tf.equal(pred,tf.reduce_max(pred,1,keep_dims = True)))\n",
    "    y = y / tf.reduce_sum(y,1,keep_dims = True)\n",
    "    \n",
    "    loss = model_loss(y,pred)\n",
    "    # compute gradient\n",
    "    \n",
    "    grad, = tf.gradients(loss,x)\n",
    "    \n",
    "    adv_x = tf.stop_gradient(x + epsilon * tf.sign(grad))\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adv = fast_gradient(x,pred,epsilon=0.07)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
